\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib,final]{nips_2018}
\usepackage[numbers]{natbib}
\usepackage{svg}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithmic,algorithm,setspace}
% \usepackage{amsfonts}
% \usepackage{amssymb}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{commath}
\usepackage{dsfont}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{subfig}
% \usepackage[utf8]{inputenc}
% \usepackage[english]{babel}
% \usepackage{caption}
% \usepackage{subcaption}

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollaries}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{property}{Property}
\newtheorem*{remark}{Remark}
\usetikzlibrary{shapes,arrows}
\usepackage{standalone}
% \usepackage[boxed]{algorithm2e}
\newcommand{\param}{\beta}
\newcommand{\s}{\mathcal{S}}
\newcommand{\p}{\mathcal{P}}
\newcommand{\pol}{\pi}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\V}{v}
\newcommand{\Vr}{\V_{\param}}
\newcommand{\regT}{\T^{\pol}_{\param}}
\newcommand{\regTo}{\T^{*}_{\param}}
\newcommand{\expect}{\mathop{\mathbb{E}}}
\newcommand{\RE}{r}

\title{Temporal Regularization in Markov Decision Process}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Pierre Thodoroff \\
  McGill University\\
  \texttt{pierre.thodoroff@mail.mcgill.ca} \\
   \And
   Audrey Durand \\
   McGill University \\
   \texttt{audrey.durand@mcgill.ca } \\
   \AND
   Joelle Pineau \\
   McGill University \& Facebook AI Research\\
   \texttt{jpineau@cs.mcgill.ca} \\
   \And
   Doina Precup \\
   McGill University \\
   \texttt{dprecup@cs.mcgill.ca} \\
}

\begin{document}
% \nipsfinalcopy is no longer used


\maketitle
\begin{abstract}
Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization.  Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories.  This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.

\end{abstract}

\section{Introduction}
There has been much progress in Reinforcement Learning (RL) techniques, with some impressive success with games~\citep{silver16}, and several interesting applications on the horizon~\citep{koedinger18,shortreed11,prasad17,dhingra17}. However RL methods are too often hampered by high variance, whether due to randomness in data collection, effects of initial conditions, complexity of learner function class, hyper-parameter configuration, or sparsity of the reward signal~\citep{henderson18}.
Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some (smaller) bias.  Regularization typically takes the form of smoothing over the observation space to reduce the complexity of the learner's hypothesis class.

In the RL setting, we have an interesting opportunity to consider an alternative form of regularization, namely \emph{temporal regularization}.  Effectively, temporal regularization considers smoothing over the trajectory, whereby the estimate of the value function at one state is assumed to be related to the value function at the state(s) that typically occur before it in the trajectory. This structure arises naturally out of the fact that the value at each state is estimated using the Bellman equation. The standard Bellman equation clearly defines the dependency between value estimates. In temporal regularization, we amplify this dependency by making each state depend more strongly on estimates of \emph{previous} states as opposed to multi-step methods that considers future states. 

This paper proposes a class of temporally regularized value function estimates. We discuss properties of these estimates, based on notions from Markov chains, under the policy evaluation setting, and extend the notion to the control case.
Our experiments show that temporal regularization effectively reduces variance and estimation error in discrete and continuous MDPs.  The experiments also highlight that regularizing in the time domain rather than in the spatial domain allows more robustness to cases where state features are mispecified or noisy, as is the case in some Atari games.



\section{Related work}
Regularization in RL has been considered via several different perspectives. One line of investigation focuses on regularizing the features learned on the state space \citep{massoud2009regularized,petrik2010feature,pazis2011non,farahmand2011regularization,liu2012regularized,harrigan2016deep}. In particular backward bootstrapping method's can be seen as regularizing in feature space based on temporal proximity \cite{sutton2009fast,li2008worst,baird1995residual}.  These approaches assume that nearby states in the state space have similar value. Other works focus on regularizing the changes in policy directly. Those approaches are often based on entropy methods \citep{neu2017unified,schulman2017proximal,bartlett2009regal}.
Explicit regularization in the temporal space has received much less attention.
Temporal regularization in some sense may be seen as a ``backward'' multi-step method \citep{sutton1998reinforcement}.  The closest work to ours is possibly \citep{xu2017natural}, where they define natural value approximator by projecting the previous states estimates by adjusting for the reward and $\gamma$. Their formulation, while sharing similarity in motivation, leads to different theory and algorithm.
Convergence properties and bias induced by this class of methods were also not analyzed in \citet{xu2017natural}.
\section{Technical Background}

\subsection{Markov chains}
We begin by introducing discrete Markov chains concepts that will be used to study the properties of temporally regularized MDPs. A discrete-time Markov chain \citep{levin2017markov} is defined by a discrete set of states $\s$ and a transition function $\p : \s \times \s \mapsto [0,1]$ which can also be written in matrix form as $P_{ij} = \p(i|j)$.
Throughout the paper, we make the following mild assumption on the Markov chain:
\begin{assumption}
The Markov chain P is ergodic: P has a unique stationary distribution $\mu$.
\end{assumption}
In Markov chains theory, one of the main challenge is to study the mixing time of the chain~\citep{levin2017markov}. Several results have been obtained when the chain is called reversible, that is when it satisfies detailed balance.
\begin{definition}[Detailed balance~\citep{kemeny1976finite}]
Let $P$ be an irreducible Markov chain with invariant stationary distribution $\mu$\footnote{$\mu_i$ defines the $i$th element of $\mu$}. A chain is said to satisfy detailed balance if and only if
\begin{equation}
    \mu_i P_{ij} = \mu_j P_{ji} \qquad \forall i,j \in \s.
\end{equation}
\end{definition}
Intuitively this means that if we start the chain in a stationary distribution, the amount of probability that flows from $i$ to $j$ is equal to the one from $j$ to $i$. In other words, the system must be at equilibrium. An intuitive example of a physical system not satisfying detailed balance is a snow flake in a coffee.
%COMMENT(pierre-luc)(snow flake not clear). 
Indeed, many chains do not satisfy this detailed balance property. In this case it is possible to use a different, but related, chain called the reversal Markov chain to infer mixing time bounds~\citep{chung2012chernoff}.
%COMMENT(pierre-luc): for these two definitions, you can cite Kemeny & Snell Finite Markov Chains (1976) 
\begin{definition}[Reversal Markov chain~\citep{kemeny1976finite}]
Let $\widetilde{P}$ the reversal Markov chain of $P$ be defined as:
\begin{equation}
    \widetilde{P_{ij}} = \frac{\mu_j P_{ji}}{\mu_i} \qquad \forall i,j \in \s.
\end{equation}
If $P$ is irreducible with invariant distribution $\mu$, then $\widetilde{P}$ is also irreducible with invariant distribution $\mu$.
\end{definition}

The reversal Markov chain $\widetilde{P}$ can be interpreted as the Markov chain $P$ with time running backwards. If the chain is reversible, then $P=\widetilde{P}$. 


\subsection{Markov Decision Process}
%COMMENT(pierre-luc): you use \P for both P(s'|s) and P(s'|s,a). Do you need a special notation ? Just "P" would be sufficient IMO.
A Markov Decision Process (MDP), as defined in \cite{puterman2014markov}, consists of a discrete set of states $\s$, a transition function $\p : \s \times \A \times \s \mapsto [0,1]$, and a reward function $\RE : \s \times \A \mapsto \R$. On each round $t$, the learner observes current state $s_t\in\s$ and selects action $a_t\in\A$, after which it receives reward $r_t = \RE(s_t,a_t)$ and moves to new state $s_{t+1}\sim \p(\cdot|s_t, a_t)$. We define a stationary policy $\pol$ as a probability distribution over actions conditioned on states $\pol : \s \times \A \mapsto [0,1]$.

\subsubsection{Discounted Markov Decision Process}
%COMMENT(pierre-luc) : I would merge this section with the above. Both sections are pretty small and there is no need to separate them as subsubsections.
When performing policy evaluation in the discounted case, the goal is to estimate the discounted expected return of policy $\pol$ at a state $s\in\s$,  $\V^{\pol}(s) = \expect_{\pol}[\sum_{t=0}^{\infty} \gamma^t \RE_{t+1} | s_0 = s]$, with discount factor $\gamma \in [0,1)$. 
This $\V^{\pol}$ can be obtained as the fixed point of the Bellman operator $\T^{\pol}$ such that:
\begin{equation}
    \T^{\pol} v^{\pol} = r^{\pol} + \gamma P^{\pol}v^{\pol},
\end{equation}
where $P^{\pol}$ denotes the $|\s|\times |\s|$ transition matrix under policy $\pol$, $v^\pi$ is the state values column-vector, and $r$ is the reward column-vector. The matrix $P^{\pol}$ also defines a Markov chain.

In the control case, the goal is to find the optimal policy $\pol^*$ that maximizes the discounted expected return. Under the optimal policy, the optimal value function $\V^*$ is the fixed point of the non-linear optimal Bellman operator:
\begin{equation}
    \T^* v^* = \max_{a \in \A} [r(a) + \gamma P(a)v^*].
\end{equation}
%COMMENT(pierre-luc)(not necessary to say non-linear)

\section{Temporal regularization}
\label{sec:temp_reg}
%COMMENT(pierre-luc): "spatial" is not common terminology perhaps better to talk about regularization in feature space/state space, as in state abstraction vs temporal abstraction. Citations are also needed. Temporal regularization : could perhaps think of entropy-regularized work as a form of temporal regularization because it regularizes over pi_t and pi_{t+1}. You may also want to make it clear that "temporal regularization" is an expression coming from you, or otherwise provide citations. 
Regularization in the feature/state space, or \emph{spatial regularization} as we call it, exploits the regularities that exist in the observation (or state). In contrast, \emph{temporal regularization} considers the temporal structure of the value estimates through a trajectory. Practically this is done by smoothing the value estimate of a state using estimates of states that occurred earlier in the trajectory.
In this section we first introduce the concept of temporal regularization and discuss its properties in the policy evaluation setting. We then show how this concept can be extended to exploit information from the entire trajectory by casting temporal regularization as a time series prediction problem. 

Let us focus on the simplest case where the value estimate at the current state is regularized using only the value estimate at the previous state in the trajectory, yielding updates of the form: % COMMENT(pierre-luc) : this sentence could be clearer. 
% Attempt: As a first step, we consider the case where the expected discounted return at a state also depends on the value at the previous step. 
% COMMENT(pierre-luc): You also need some more motivation as to why this particular form, especially the time-reversal aspect. Otherwise, it seems to be taken totally out of the blue. Why time-reversal matrix and not just P ? 
% COMMENT(pierre-luc) : "As a first step" suggests that you will introduce other forms of this operator in the paper. You do present the "time series" generalization, but don't develop it much further. So I would get rid of "first step", and start directly by explaining why incorporating previous values might be a good idea. 
\begin{equation}
    \begin{split}
        \Vr(s_{t}) &= \expect_{s_{t+1},s_{t-1} \sim \pi} [r(s_t) + \gamma ((1-\param)\Vr(s_{t+1}) + \param \Vr(s_{t-1}))]\\
         &= r(s_t) + \gamma (1-\param)\sum_{s_{t+1}\in \s} p(s_{t+1}|s_t)\Vr(s_{t+1})
         + \gamma \param \sum_{s_{t-1} \in \s}\frac{p(s_t|s_{t-1}) p(s_{t-1})}{p(s_t)} \Vr(s_{t-1}),
    \end{split}
\end{equation}
for a parameter $\param \in [0,1]$ and $p(s_{t+1}|s_t)$ the transition probability induced by the policy $\pol$. It can be rewritten in matrix form as $\Vr = r + \gamma (((1-\param) P^{\pol} + \param \widetilde{P^{\pol}})\Vr) $, where $\widetilde{P^{\pol}}$ corresponds to the reversal Markov chain of the MDP.
We define a temporally regularized Bellman operator as:
\begin{equation}
\label{eqn:temp_reg_bellman_op}
    \regT v_{\param} = r + \gamma ((1-\param) P^{\pol}v_{\param} + \param \widetilde{P^{\pol}} v_{\param}).
\end{equation}
To alleviate the notation, we denote $P^{\pol}$ as $P$ and $\widetilde{P^{\pol}}$ as $\widetilde{P}$.
\begin{remark}
For $\param =0$, Eq.~\ref{eqn:temp_reg_bellman_op} corresponds to the original Bellman operator.
\end{remark}
We can prove that this operator has the following property.
%COMMENT(pierre-luc): I would just splash Theorem 1 without stating Lemma 1 (which is just one line and isn't proved). You can instead mention it in the proof, and give a reference for this fact.
% \begin{lemma}
% \label{lem:stochastic_matrix_convex_combination}
% The convex combination of two row stochastic matrices is also row stochastic.
% \end{lemma}
\begin{theorem}
The operator $\regT$ has a unique fixed point $\Vr^{\pol}$ and $\regT$ is a contraction mapping.
\end{theorem}
\begin{proof}
We first prove that $\regT$ is a contraction mapping in $L_\infty$ norm. We have that
\begin{equation}
\begin{split}
    \norm{\regT u - \regT v}_{\infty} &= \norm{r+\gamma( (1-\param) Pu + \param \widetilde{P}u) - (r+\gamma( (1-\param) Pv + \param \widetilde{P}v))}_{\infty}\\
    &= \gamma \norm{((1-\param) P + \param \widetilde{P})(u-v)}_{\infty}\\
    &\leq \gamma \norm{u-v}_{\infty},
\end{split}
\end{equation}
where the last inequality uses the fact that the convex combination of two row stochastic matrices is also row stochastic (the proof can be found in the appendix).
%is obtained with Lemma~\ref{lem:stochastic_matrix_convex_combination}.
Then using Banach fixed point theorem, we obtain that $\Vr^{\pol}$ is a unique fixed point. 
\end{proof}
Furthermore the new induced Markov chain $(1-\param) P + \param \widetilde{P}$ has the same stationary distribution as the original $P$ (the proof can be found in the appendix).
\begin{lemma}
\label{cor:same_stationary_dist}
$P$ and $(1-\param) P + \param \widetilde{P}$ have the same stationary distribution $\mu \quad \forall \param \in [0,1]$.
\end{lemma}
%COMMENT(pierre-luc): $P$ and $(1-\param) : order is confusing. I would write something like:
% The stationary distribution induced by $(1-\param) P + \param \widetilde{P}$ is $\mu$: the stationary distribution under the original Markov chain $P$.


%COMMENT(pierre-luc): You first need to start this paragraph by saying that there is indeed a bias, because this is not something that you would expect from a "regular" policy evaluation operator. Then quickly reinsure the reader that this bias is not evil because the resulting operator as some desirable properties : a knob for variance reduction. 
In the policy evaluation setting, the bias between the original value function $\V^{\pol}$ and the regularized one $\V^{\pol}_{\param}$ can be characterized as a function of the difference between $P$ and its Markov reversal $\widetilde{P}$, weighted by $\param$ and the reward distribution.

%COMMENT(pierre-luc): You need to say that these Neumann series converge. This follows from your contraction argument above. You can state Puterman corollary C.4 for the existence of the series and its inverse. 

\begin{proposition}
Let $v^{\pol} = \sum_{i=0}^{\infty} \gamma^i P^i r$ and $v^{\pol}_{\param} = \sum_{i=0}^{\infty} \gamma^i ((1-\param) P + \param \widetilde{P})^i r$. We have that
\begin{equation}
    \begin{split}
        \norm{v^{\pol} - v^{\pol}_{\param}}_{\infty} &= \norm{\sum_{i=0}^{\infty} \gamma^i (P^i-((1-\param) P + \param \widetilde{P})^i) r}_{\infty}
        \leq \sum_{i=0}^{\infty} \gamma^i \norm{ (P^i-((1-\param) P + \param \widetilde{P})^i) r}_{\infty}.
    \end{split}
\end{equation}
This quantity is naturally bounded for $\gamma < 1$.
\end{proposition}
\begin{remark}
Let $P^\infty$ denote a matrix where columns consist of the stationary distribution $\mu$.
By the property of reversal Markov chains and lemma \ref{cor:same_stationary_dist}, we have that $\lim_{i\rightarrow\infty} \Vert P^i r-P^\infty r \Vert \rightarrow 0$ and $\lim_{i\rightarrow\infty} \Vert ((1-\param)P+\param \widetilde{P})^i r-P^\infty r \Vert \rightarrow 0$, such that the Marvov chain $P$ and its reversal $(1-\param)P+\param \widetilde{P}$ converge to the same value. Therefore, the norm $\Vert  (P^i-((1-\param) P + \param \widetilde{P})^i) r \Vert_p$ also converges to 0 in the limit.
\end{remark}

\begin{remark}
It can be interesting to note that if the chain is reversible, meaning that $P = \widetilde{P}$, then the fixed point of both operators is the same, that is $v^\pol = v_\param^\pol$.
\end{remark}

\paragraph{Discounted average reward case:} The temporally regularized MDP has the same discounted average reward as the original one as it is possible to define the discounted average reward \citep{tsitsiklis2002average} as a function of the stationary distribution $\pi$, the reward vector and $\gamma$ . This leads to the following property (the proof can be found in the appendix).

\begin{proposition}
For a reward vector r, the MDPs defined by the the transition matrices $P$ and $(1-\param) P + \param \widetilde{P}$ have the same average reward $\rho$.
\end{proposition}
Intuitively, this means that temporal regularization only reweighs the reward on each state based on the Markov reversal, while preserving the average reward.

%COMMENT(pierre-luc): You could also call this section "Higher Order Temporal Regularization". In RLS/ARMA/signal processing, I think they refer to the "order" of their filter when they incorporate older information. See LTI filters.
\paragraph{Temporal Regularization as a time series prediction problem:}
It is possible to cast this problem of temporal regularization as a time series prediction problem, and use richer models of temporal dependencies, such as exponential smoothing \citep{gardner2006exponential}, ARMA model~\citep{box94}, etc. We can write the update in a general form using $n$ different regularizers ($\widetilde{v_0},\widetilde{v_1}...\widetilde{v_{n-1}}$):
\begin{equation}
    \V(s_t) = r(s) + \gamma \sum_{i=0}^{n-1} [\param(i) \widetilde{\V}_i(s_{t+1})],
\end{equation}
where $\widetilde{\V}_0(s_{t+1}) = \V(s_{t+1})$ and $\sum_{i=0}^{n-1} \param(i) = 1$. For example, using exponential smoothing where $\widetilde{\V}(s_{t+1}) = (1-\lambda) \V(s_{t-1}) + (1-\lambda)\lambda \V(s_{t-2})...$, the update can be written in operator form as:
\begin{equation}
    \label{eq:exp_smooth}
    \regT v = r + \gamma \bigg(\left(1-\param\right) Pv + \param \left(1-\lambda\right) \sum_{i=1}^{\infty} \lambda^{i-1} \widetilde{P}^i v\bigg),
\end{equation}
and a similar argument as Theorem 1 can be used to show the contraction property. The bias of exponential smoothing in policy evaluation can be characterized as:
\begin{equation}
        \norm{v^{\pol} - v^{\pol}_{\param}}_{\infty} \leq  \sum_{i=0}^{\infty} \gamma^i \norm{ (P^i-((1-\param) P + \param (1-\lambda) \sum_{j=1}^{\infty} \lambda^{j-1} \widetilde{P}^j)^i) r}_{\infty}.
\end{equation}

Using more powerful regularizers could be beneficial, for example to reduce variance by smoothing over more values (exponential smoothing) or to model the trend of the value function through the trajectory using trend adjusted model ~\cite{gardner1985exponential}. An example of a temporal policy evaluation with temporal regularization using exponential smoothing is provided in Algorithm \ref{alg:pol_eval_exp_smoothing}.
\begin{algorithm}[H]
\caption{Policy evaluation with exponential smoothing}
\begin{spacing}{1.2}
\begin{algorithmic}[1]
    \STATE Input: $\pi,\alpha,\gamma,\param,\lambda$
    \STATE $p = \V(s)$
    \FORALL{steps}
        \STATE Choose $a \sim \pi(s)$
        \STATE Take action $a$, observe reward $r(s)$ and next state $s'$
        \STATE $\V(s) = \V(s) +  \alpha (r(s) + \gamma ( (1-\param) \V(s') + \param p)) $
        \STATE $p = (1-\lambda)\V(s) + \lambda p$
    \ENDFOR
\end{algorithmic}
\end{spacing}
\label{alg:pol_eval_exp_smoothing}
\end{algorithm}

%COMMENT(pierre-luc) : It feels too disconnected from the flow. I would just move this to the conclusion as future work. 
\paragraph{Control case:} 
Temporal regularization can be extended to MDPs with actions by  modifying the target of the value function (or the Q values) using temporal regularization. Experiments (Sec.~\ref{sec:expe:drl}) present an example of how temporal regularization can be applied within an actor-critic framework. The theoretical analysis of the control case is outside the scope of this paper.

\paragraph{Temporal difference with function approximation:}
It is also possible to extend temporal regularization using function approximation such as  semi-gradient TD \cite{sutton2017reinforcement}. 
%COMMENT(pierre-luc): Semi-gradient is a new terminology introduced in the draft of the new S&B textbook. Not found in 1998
Assuming a function $\V_{\theta}^{\param}$ parameterized by $\theta$, we can consider $r(s) + \gamma ((1-\param)\V_{\theta}^{\param}(s_{t+1}) + \param \V_{\theta}^{\param}(s_{t-1})) - \V_{\theta}^{\param}(s_t)$  as the target and differentiate with respect to $\V_{\theta}^{\param}(s_{t})$. An example of a temporally regularized semi-gradient TD algorithm can be found in the appendix.
%COMMENT(pierre-luc) : I would include this in the main text + pseudo-code. Important to show that the MDP formulation can be efficiently translated into a usable model-free algorithm. The time-reversibility would be a concern for me if I were to review this paper and it hadn't be addressed by this point yet. 


\section{Experiment}
We now presents empirical results illustrating potential advantages of temporal regularization, and characterizing its bias and variance effects on value estimation and control.



\subsection{Mixing time}
This first experiment showcases that the underlying Markov chain of a MDP can have a smaller mixing time when temporally regularized. The mixing time can be seen as the number of time steps required for the Markov chain to get \emph{close enough} to its stationary distribution. Therefore, the mixing time also determines the rate at which policy evaluation will converge to the optimal value function~\citep{baxter2001infinite}. 
%COMMENT(pierre-luc) : can also cite Puterman, spectral radius. Also, not just for control (you say "optimal") : also true for policy evaluation.
%COMMENT(pierre-luc): Mixing quickly is one thing, but mixing quickly and returning an answer not too far away from the actual answer (bias) is very important. When reading this paragraph, I would need to convince myself that this is not a vacuous demonstration, that the resulting fast-mixing chain is still somewhat related to the original problem.
We consider a synthetic MDP with 10 states where transition probabilities are sampled from the uniform distribution. Let $P^{\infty}$ denote a matrix where columns consists of the stationary distribution $\mu$. To compare the mixing time, we evaluate the error corresponding to the distance of $P^i$ and $\big((1-\param)P+\param \widetilde{P}\big)^i$ to the convergence point $P^{\infty}$ after $i$ iterations.
%
Figure~\ref{fig:mixing} displays the error curve when varying the regularization parameter $\param$. We observe a U-shaped error curve, that intermediate values of $\param$ in this example yields faster mixing time.
One explanation is that transition matrices with extreme probabilities (low or high) yield poorly conditioned transition matrices. Regularizing with the reversal Markov chain often leads to a better conditioned matrix at the cost of injecting bias.
%COMMENT(pierre-luc) : poorly conditioned: you should compute the actual condition numbers to convince the reader that this is actually what is happening. 

\subsection{Bias}
\label{sec:expe:bias}

It is well known that reducing variance comes at the expense of inducing (smaller) bias.
%COMMENT(pierre-luc): citation still needed. You can perhaps cite general ML textbooks such as Hastie ESL or Bishop. You can also cite  Bertsekas (2012) and Satinder Singh and Kearns for bias-variance of TD(lambda).
This has been characterized previously (Sec.~\ref{sec:temp_reg}) in terms of the difference between the original Markov chain and the reversal weighted by the reward. In this experiment, we attempt to give an intuitive idea of what this means. More specifically, we would expect the bias to be small if values along the trajectories have similar values.
To this end, we consider a synthetic MDP with $10$ states where both transition functions and rewards are sampled randomly from a uniform distribution. In order to create temporal dependencies in the trajectory, we smooth the rewards of $N$ states that are temporally close (in terms of trajectory) using the following formula: $ r(s_t) = \frac{r(s_t) + r(s_{t+1})}{2}$.
Figure~\ref{fig:mod} shows the difference between the regularized and un-regularized MDPs as $N$ changes, for different values of regularization parameter $\param$.
We observe that increasing $N$, meaning more states get rewards close to one another, results into less bias. This is due to rewards putting emphasis on states where the original and reversal Markov chain are similar.

\begin{figure}
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[height=5cm,width=\linewidth]{Markov_mixing.pdf}
\caption{Distance between the stationary transition probabilities and the estimated transition probability for different values of regularization parameter $\param$.}
\label{fig:mixing}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[height=5.2cm,width=\linewidth]{Bias_N.pdf}
\caption{Mean difference between $\Vr^{\pi}$ and $\V^{\pi}$ given the regularization parameter $\param$, for different amount of smoothed states $N$.}
\label{fig:mod}
\end{minipage}
\end{figure}

\subsection{Variance}
\label{sec:expe:variance}
% COMMENT(pierre-luc) : this should be emphasized earlier.
The primary motivation of this work is to reduce variance, therefore we now consider an experiment  targeting this aspect. Figure~\ref{fig:MDP} shows an example of a synthetic, 3-state MDP, where the variance of $S_1$ is (relatively) high. We consider an agent that is evolving in this world, changing states following the stochastic policy indicated. We are interested in the error when estimating the optimal state value of $S_1$, $\V^*(S_1)$, with and without temporal regularization, denoted $\Vr^{\pi}(S_1)$, $\V^{\pi}(S_1)$, respectively.

Figure~\ref{fig:perf_MDP} shows these errors at each iteration, averaged over $100$ runs. We observe that temporal regularization indeed reduces the variance and thus helps the learning process by making the value function easier to learn.


\begin{figure}
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[width=\linewidth]{Exp_2.png}
\caption{Synthetic MDP where state $S_1$ has high variance.}
\label{fig:MDP}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[width=\linewidth]{Exp_2_result.png}
\caption{Left plot shows absolute difference between original ($\V^{\pi}(S_1)$) and regularized ($\Vr^{\pi}(S_1)$) state value estimates to the optimal value $\V^*(S_1)$. Right plot shows the variance of the estimates $\V$.}
\label{fig:perf_MDP}
\end{minipage}%
\end{figure}

\subsection{Propagation of the information}
We now illustrate with a simple experiment how temporal regularization allows the information to spread faster among the different states of the MDP. For this purpose, we consider a simple MDP, where an agent walks randomly in two rooms (18 states) using four actions (up, down, left, right), and a discount factor $\gamma=0.9$. The reward is $r_t=1$ everywhere and passing the door between rooms (shown in red on Figure~\ref{fig:room}) only happens 50\% of the time (on attempt). The episode starts at the top left and terminates when the agent reaches the bottom right corner. The sole goal is to learn the optimal value function by walking along this MDP (this is not a race toward the end).

Figure~\ref{fig:room} shows the proximity of the estimated state value to the optimal value with and without temporal regularization. The darker the state, the closer it is to its optimal value. The heatmap scale has been adjusted at each trajectory to observe the difference between both methods.
We first notice that the overall propagation of the information in the regularized MDP is faster than in the original one. We also observe that, when first entering the second room, bootstrapping on values coming from the first room allows the agent to learn the optimal value faster. This suggest that temporal regularization could help agents explore faster by using their prior from the previous visited state for learning the corresponding optimal value faster. It is also possible to consider more complex and powerful regularizers. Let us study a different time series prediction model, namely exponential averaging, as defined in (\ref{eq:exp_smooth}). The complexity of such models is usually articulated by hyper-parameters, allowing complex models to improve performance by better adapting to problems. We illustrate this by comparing the performance of regularization using the previous state and an exponential averaging of all previous states. Fig.~\ref{fig:room_perf} shows the average error on the value estimate using past state smoothing, exponential smoothing, and without smoothing. In this setting, exponential smoothing transfers information faster, thus enabling faster convergence to the true value.

\begin{figure}
    \centering
    \includegraphics[scale=0.44]{room.png}
    \caption{Proximity of the estimated state value to the optimal value after $N$ trajectories. Top row is the original room environment and bottom row is the regularized one ($\param = 0.5$). Darker is better.}
    \label{fig:room}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.60]{room_perf.pdf}
\caption{Benefits of complex regularizers on the room domain.}
\label{fig:room_perf}
\end{figure}



\subsection{Noisy state representation}
\label{sec:expe:noisy_state}

The next experiment illustrates a major strength of temporal regularization, that is its robustness to noise in the state representation. This situation can naturally arise when the state sensors are noisy or insufficient to avoid aliasing. For this task, we consider the synthetic, one dimensional, continuous setting. A learner evolving in this environment walks randomly along this line with a discount factor $\gamma=0.95$. Let $x_t\in [0,1]$ denote the position of the agent along the line at time $t$. The next position $x_{t+1} = x_t + a_t$, where action $a_t\sim\mathcal{N}(0, 0.05)$. The state of the agent corresponds to the position perturbed by a zero-centered Gaussian noise $\epsilon_t$, such that $s_t = x_t + \epsilon_t$, where $\epsilon_t\sim\mathcal{N}(0,\sigma^2)$ are i.i.d. When the agent moves to a new position $x_{t+1}$, it receives a reward $r_t = x_{t+1}$. The episode ends after 1000 steps. In this experiment we model the value function using a linear model with a single parameter $\theta$. We are interested in the error when estimating the optimal parameter function $\theta^*$ with and without temporal regularization, that is $\theta^\pi_\param$ and $\theta^\pi$, respectively. In this case we use the TD version of temporal regularization presented at the end of Sec.~\ref{sec:temp_reg}.
Figure~\ref{fig:perf_MDP_noisy} shows these errors, averaged over 1000 repetitions, for different values of noise variance $\sigma^2$. We observe that as the noise variance increases, the un-regularized estimate becomes less accurate, while temporal regularization is more robust. Using more complex regularizer can improve performance as shown in the previous section but this potential gain comes at the price of a potential loss in case of model misfit. Fig.~\ref{fig:smoothing} shows the absolute distance from the regularized state estimate (using exponential smoothing) to the optimal value while varying $\lambda$ (higher $\lambda$ = more smoothing). Increasing smoothing improves performance up to some point, but when $\lambda$ is not well fit the bias becomes too strong and performance declines. This is a classic bias-variance tradeoff. This experiment highlights a case where temporal regularization is effective even in the absence of smoothness in the state space (which other regularization methods would target). This is further highlighted in the next experiments.

\begin{figure}
\begin{minipage}[c]{0.45\linewidth}
\centering
\includegraphics[height=4.5cm,width=\linewidth]{noisy_walk.pdf}
\caption{Absolute distance from the original ( $\theta^\pi$) and the regularized ($\theta^\pi_\param$) state value estimates to the optimal parameter $\theta^*$ given the noise variance $\sigma^2$ in state sensors.}
\label{fig:perf_MDP_noisy}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\linewidth}
\centering
\includegraphics[height=4.5cm,width=\linewidth]{error_noisy.pdf}
\caption{Impact of complex regularizer parameterization ($\lambda$) on the noisy walk using exponential smoothing.}
\label{fig:smoothing}
\end{minipage}%
\end{figure}

\subsection{Deep reinforcement learning}
\label{sec:expe:drl}

To showcase the potential of temporal regularization in high dimensional settings, we adapt an actor-critic based method (PPO \citep{schulman2017proximal}) using temporal regularization. More specifically, we incorporate temporal regularization as exponential smoothing in the target of the critic.  PPO uses the general advantage estimator $\hat{A}_t = \delta_t + \gamma \lambda \delta_{t+1} + ... + (\gamma \lambda)^{T-t+1} \delta_{T}$ where $\delta_t = r_t + \gamma v(s_{t+1}) - v(s_{t})$. We regularize $\delta_t$ such that $\delta_t^{\beta} = r_t + \gamma ((1-\beta)v(s_{t+1}) + \beta \widetilde{v}(s_{t-1}))) - v(s_{t})$ using exponential smoothing $\widetilde{v}(s_{t}) = (1-\lambda)v(s_t) + \lambda \widetilde{v}(s_{t-1})$ as described in Eq.~(\ref{eq:exp_smooth}). $\widetilde{v}$ is an exponentially decaying sum over all $t$ previous state values encountered in the trajectory.
We evaluate the performance in the Arcade Learning Environment ~\cite{bellemare2013arcade}, where we consider the following performance measure:
\begin{equation}
\label{eqn:expe:drl_measure}
    \frac{\text{regularized} - \text{baseline}}{\text{baseline} - \text{random}}.
\end{equation}
%COMMENT(pierre-luc) : what is "baseline", plain PPO that you re-ran or other reported algorithm ?
The hyper-parameters for the temporal regularization are $\param = \lambda = 0.2$ and a decay of $1\mathrm{e}^{-5}$. Those are selected on 7 games and 3 training seeds. All other hyper-parameters correspond to the one used in the PPO paper. Our implementation\footnote{The code can be found \url{https://github.com/pierthodo/temporal_regularization}.} is based on the publicly available OpenAI codebase~\cite{baselines}. The previous four frames are considered as the state representation~\citep{mnih2015human}.
For each game, $10$ independent runs ($10$ random seeds) are performed. 


The results reported in Figure~\ref{fig:graph_atari} show that adding temporal regularization improves the performance on multiple games. This suggests that the regularized optimal value function may be smoother and thus easier to learn, even when using function approximation with deep learning. Also, as shown in previous experiments (Sec.~\ref{sec:expe:noisy_state}), temporal regularization being independent of spatial representation makes it more robust to mis-specification of the state features, which is a challenge in some of these games (e.g. when assuming full state representation using some previous frames).

\begin{figure}
    \centering
    \includegraphics[width=13cm,height=4cm]{bar_atari.png}
    \caption{Performance (Eq.~\ref{eqn:expe:drl_measure}) of a temporally regularized PPO on a suite of Atari games.}
    \label{fig:graph_atari}
\end{figure}


\section{Discussion}

\paragraph{Noisy states:}
%COMMENT(pierre-luc): Get rid of "people". Instead: "It is often assumed that[...]" + citation to support "often". 
Is is often assumed that the full state can be determined, while in practice, the Markov property rarely holds. This is the case, for example, when taking the four last frames to represent the state in Atari games~\citep{mnih2015human}. 
%COMMENT(pierre-luc) : last four frame, cite DQN paper where this was first introduced.
A problem that arises when treating a partially observable MDP (POMDP) as a fully observable is that it may no longer be possible to assume that the value function is smooth over the state space~\citep{singh1994learning}.
%COMMENT(pierre-luc): Need citation here. Perhaps "Learning Without State-Estimation in Partially Observable Markovian Decision Processes" by Singh 1994 for general phenomenon of "aliasing" when using function approximation. 
For example, the observed features may be similar for two states that are intrinsically different, leading to highly different values for states that are nearby in the state space. Previous experiments on noisy state representation (Sec.~\ref{sec:expe:noisy_state}) and on the Atari games (Sec.~\ref{sec:expe:drl}) show that temporal regularization provides robustness to those cases. This makes it an appealing technique in real-world environments, where it is harder to provide the agent with the full state.
% COMMENT(pierre-luc) : the paper "On the Role of Tracking in Stationary Environments" by Sutton, Koop, Silver (2009) is related to this idea of "temporal smoothing". They develop the idea that "tracking" can be useful under partial observability (extremenly improverished features). In fact, their "Black and white" world would have been a nice experiment to try in this paper.
% COMMENT(pierre-luc) two other related ideas about "temporal smoothing. Options : I showed in the AAAI (2017) and in my thesis that Markov options + call return can be interpreted mathematically as a special mixture model where P(o' | s', o) = (1 - B(s',o))1_{o'=o} + B(s',o)mu(o'|s'). It's also a form of "temporal smoothing" where (1-B) determines how much you want to bias your distribution over options given by mu. The more you "continue" (B goes to 0), the more you "commit" and the more you "smooth". 
% COMMENT(pierre-luc) : Another related idea is from Marc and his " Persistent Q-Learning" algorithm in "Increasing the Action Gap: New Operators for Reinforcement Learning". Very similar structure as Markov options (but MDP setting). 
\paragraph{Choice of the regularization parameter:}

The bias induced by the regularization parameter $\param$ can be detrimental for the learning in the long run. A first attempt to mitigate this bias is just to decay the regularization as learning advances, as it is done in the deep learning experiment (Sec.~\ref{sec:expe:drl}). Among different avenues that could be explored, an interesting one could be to aim for a state dependent regularization. For example, in the tabular case, one could consider $\param$ as a function of the number of visits to a particular state.
% COMMENT(pierre-luc): could perhaps be cast as dual averaging. See Neu paper
\paragraph{Smoother objective:}

%COMMENT(pierre-luc) : "shape" is vague
Previous work~\cite{smooth_value} looked at how the smoothness of the objective function relates to the convergence speed of RL algorithms.  An analogy can be drawn with convex optimization where the rate of convergence is dependent on the Lipschitz (smoothness) constant ~\cite{boyd2004convex}. 
By smoothing the value temporally we argue that the optimal value function can be smoother. This would be beneficial in high-dimensional state space where the use of deep neural network is required. This could explain the performance displayed using temporal regularization on Atari games (Sec.~\ref{sec:expe:drl}).
The notion of temporal regularization is also behind multi-step methods~\citep{sutton1998reinforcement}; it may be worthwhile to further explore how these methods are related.

\paragraph{Conclusion:}

This paper tackles the problem of regularization in RL from a new angle, that is from a temporal perspective. In contrast with typical spatial regularization, where one assumes that rewards are close for nearby states in the state space, temporal regularization rather assumes that rewards are close for states \emph{visited closely in time}. This approach allows information to propagate faster into states that are hard to reach, which could prove useful for exploration. The robustness of the proposed approach to noisy state representations and its interesting properties should motivate further work to explore novel ways of exploiting temporal information.

\subsubsection*{Acknowledgments}
The authors wish to thank Pierre-Luc Bacon, Harsh Satija and Joshua Romoff  for helpful discussions. Financial support was provided by NSERC and Facebook. This research was enabled by support provided by Compute Canada. We thank the reviewers for insightful comments and suggestions.


\clearpage



\bibliographystyle{abbrvnat}
\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{baird1995residual}
L.~Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
P.~L. Bartlett and A.~Tewari.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 35--42. AUAI Press, 2009.

\bibitem[Baxter and Bartlett(2001)]{baxter2001infinite}
J.~Baxter and P.~L. Bartlett.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Box et~al.(1994)Box, Jenkins, and Reinsel]{box94}
G.~Box, G.~M. Jenkins, and G.~C. Reinsel.
\newblock \emph{Time Series Analysis: Forecasting and Control (3rd ed.)}.
\newblock Prentice-Hall, 1994.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chung et~al.(2012)Chung, Lam, Liu, and
  Mitzenmacher]{chung2012chernoff}
K.-M. Chung, H.~Lam, Z.~Liu, and M.~Mitzenmacher.
\newblock Chernoff-hoeffding bounds for markov chains: Generalized and
  simplified.
\newblock \emph{arXiv preprint arXiv:1201.0559}, 2012.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, and Wu]{baselines}
P.~Dhariwal, C.~Hesse, O.~Klimov, A.~Nichol, M.~Plappert, A.~Radford,
  J.~Schulman, S.~Sidor, and Y.~Wu.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Dhingra et~al.(2017)Dhingra, Li, Li, Gao, Chen, Ahmed, and
  Deng]{dhingra17}
B.~Dhingra, L.~Li, X.~Li, J.~Gao, Y.-N. Chen, F.~Ahmed, and L.~Deng.
\newblock Towards end-to-end reinforcement learning of dialogue agents for
  information access.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics}, volume~1, pages 484--495, 2017.

\bibitem[Farahmand(2011)]{farahmand2011regularization}
A.-m. Farahmand.
\newblock \emph{Regularization in reinforcement learning}.
\newblock {PhD} thesis, University of Alberta, 2011.

\bibitem[Farahmand et~al.(2009)Farahmand, Ghavamzadeh, Szepesv{\'a}ri, and
  Mannor]{massoud2009regularized}
A.-m. Farahmand, M.~Ghavamzadeh, C.~Szepesv{\'a}ri, and S.~Mannor.
\newblock Regularized fitted q-iteration for planning in continuous-space
  markovian decision problems.
\newblock In \emph{American Control Conference}, pages 725--730. IEEE, 2009.

\bibitem[Gardner(2006)]{gardner2006exponential}
E.~S. Gardner.
\newblock Exponential smoothing: The state of the artâ€”part ii.
\newblock \emph{International journal of forecasting}, 22\penalty0
  (4):\penalty0 637--666, 2006.

\bibitem[Gardner~Jr(1985)]{gardner1985exponential}
E.~S. Gardner~Jr.
\newblock Exponential smoothing: The state of the art.
\newblock \emph{Journal of forecasting}, 4\penalty0 (1):\penalty0 1--28, 1985.

\bibitem[Harrigan(2016)]{harrigan2016deep}
C.~Harrigan.
\newblock Deep reinforcement learning with regularized convolutional neural
  fitted q iteration.
\newblock 2016.

\bibitem[Henderson et~al.(2018)Henderson, Islam, and Bachman]{henderson18}
P.~Henderson, R.~Islam, and P.~J. P. D. M.~D. Bachman, P.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{AAAI}, 2018.

\bibitem[Kemeny and Snell(1976)]{kemeny1976finite}
J.~G. Kemeny and J.~L. Snell.
\newblock Finite markov chains, undergraduate texts in mathematics.
\newblock 1976.

\bibitem[Koedinger et~al.(2018)Koedinger, Brunskill, Baker, and
  McLaughlin]{koedinger18}
K.~Koedinger, E.~Brunskill, R.~Baker, and E.~McLaughlin.
\newblock New potentials for data-driven intelligent tutoring system
  development and optimization.
\newblock \emph{AAAI magazine}, 2018.

\bibitem[Laroche(2018)]{smooth_value}
V.~S. Laroche.
\newblock In reinforcement learning, all objective functions are not equal.
\newblock \emph{ICLR Workshop}, 2018.

\bibitem[Levin and Peres(2008)]{levin2017markov}
D.~A. Levin and Y.~Peres.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2008.

\bibitem[Li(2008)]{li2008worst}
L.~Li.
\newblock A worst-case comparison between temporal difference and residual
  gradient with linear function approximation.
\newblock In \emph{Proceedings of the 25th international conference on machine
  learning}, pages 560--567. ACM, 2008.

\bibitem[Liu et~al.(2012)Liu, Mahadevan, and Liu]{liu2012regularized}
B.~Liu, S.~Mahadevan, and J.~Liu.
\newblock Regularized off-policy td-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  836--844, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
G.~Neu, A.~Jonsson, and V.~G{\'o}mez.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Pazis and Parr(2011)]{pazis2011non}
J.~Pazis and R.~Parr.
\newblock Non-parametric approximate linear programming for mdps.
\newblock In \emph{AAAI}, 2011.

\bibitem[Petrik et~al.(2010)Petrik, Taylor, Parr, and
  Zilberstein]{petrik2010feature}
M.~Petrik, G.~Taylor, R.~Parr, and S.~Zilberstein.
\newblock Feature selection using regularization in approximate linear programs
  for markov decision processes.
\newblock \emph{arXiv preprint arXiv:1005.1860}, 2010.

\bibitem[Prasad et~al.(2017)Prasad, Cheng, Chivers, Draugelis, and
  Engelhardt]{prasad17}
N.~Prasad, L.~Cheng, C.~Chivers, M.~Draugelis, and B.~Engelhardt.
\newblock A reinforcement learning approach to weaning of mechanical
  ventilation in intensive care units.
\newblock In \emph{UAI}, 2017.

\bibitem[Puterman(1994)]{puterman2014markov}
M.~L. Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 1994.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shortreed et~al.(2011)Shortreed, Laber, Lizotte, Stroup, Pineau, and
  Murphy]{shortreed11}
S.~Shortreed, E.~Laber, D.~Lizotte, S.~Stroup, J.~Pineau, and S.~Murphy.
\newblock Informing sequential clinical decision-making through reinforcement
  learning: an empirical study.
\newblock \emph{Machine Learning}, 2011.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Driessche,
  Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman, Grewe, Nham,
  Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel, and
  Hassabis]{silver16}
D.~Silver, A.~Huang, C.~Maddison, A.~Guez, L.~Sifre, G.~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Singh et~al.(1994)Singh, Jaakkola, and Jordan]{singh1994learning}
S.~P. Singh, T.~Jaakkola, and M.~I. Jordan.
\newblock Learning without state-estimation in partially observable markovian
  decision processes.
\newblock In \emph{Machine Learning Proceedings 1994}, pages 284--292.
  Elsevier, 1994.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press Cambridge, 1st edition, 1998.

\bibitem[Sutton and Barto(2017)]{sutton2017reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press Cambridge, (in progress) 2nd edition, 2017.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
R.~S. Sutton, H.~R. Maei, D.~Precup, S.~Bhatnagar, D.~Silver,
  C.~Szepesv{\'a}ri, and E.~Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 993--1000. ACM, 2009.

\bibitem[Tsitsiklis and Van~Roy(2002)]{tsitsiklis2002average}
J.~N. Tsitsiklis and B.~Van~Roy.
\newblock On average versus discounted reward temporal-difference learning.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 179--191, 2002.

\bibitem[Xu et~al.(2017)Xu, Modayil, van Hasselt, Barreto, Silver, and
  Schaul]{xu2017natural}
Z.~Xu, J.~Modayil, H.~P. van Hasselt, A.~Barreto, D.~Silver, and T.~Schaul.
\newblock Natural value approximators: Learning when to trust past estimates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2117--2125, 2017.

\end{thebibliography}
\clearpage
\section{Appendix}
\setcounter{lemma}{0}
\setcounter{property}{1}

\begin{lemma}
$P$ and $(1-\param) P + \param \widetilde{P}$ have the same stationary distribution $\mu \quad \forall \param \in [0,1]$.
\end{lemma}
\begin{proof}
It is known that $P^{\pol}$ and $\widetilde{P^{\pol}}$ have the same stationary distribution. Using this fact we have that
\begin{equation}
    \begin{split}
        \mu( (1-\param) P^{\pol} + \param \widetilde{P^{\pol}}) &=  (1-\param) \mu P^{\pol} + \param \mu \widetilde{P^{\pol}} \\
        &= (1-\param)\mu + \param \mu\\
        &= \mu.
    \end{split}
\end{equation}
\end{proof} 

\begin{property}
For a reward vector r, the MDP defined by the the transition matrix $P$ and $(1-\param) P + \param \widetilde{P}$ have the same discounted average reward $\rho$.
\begin{equation}
    \frac{\rho}{1-\alpha} = \sum_i^{\infty}\gamma^i \pi^T r.
\end{equation}
\end{property}
\begin{proof}
Using lemma 1, both $P$ and $(1-\param) P + \param \widetilde{P}$ have the same stationary distribution and so discounted average reward.
\end{proof}



\begin{lemma}
\label{lem:stochastic_matrix_convex_combination}
The convex combination of two row stochastic matrices is also row stochastic.
\end{lemma}
\begin{proof}
Let e be vector a columns vectors of 1.
\begin{equation}
\begin{split}
       (\param P^{\pol} + (1-\param) \widetilde{P^{\pol}})e &= \param P^{\pol}e +  (1-\param) \widetilde{P^{\pol}}e \\
       &= \param e + (1-\param) e\\
       &= e.
\end{split}
\end{equation}
\end{proof}



\begin{algorithm}[H]
\caption{Temporally regularized semi-gradient TD}
\begin{spacing}{1.2}
\begin{algorithmic}[1]
    \STATE Input: policy $\pi$,$\param$,$\gamma$
    \FORALL{steps}
        \STATE Choose $a \sim \pi(s_t)$
        \STATE Take action $a$, observe $r(s),s_{t+1}$
        \STATE $\theta = \theta + \alpha (r + \gamma((1-\param) \V_{\theta}(s_{t+1}) + \param \V_{\theta}(s_{t-1})) - \V_{\theta}(s_t))\nabla \V_{\theta}(s_{t}) $
    \ENDFOR
\end{algorithmic}
\end{spacing}
\end{algorithm}
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{plot_atari.png}
    \caption{Average reward per episode on Atari games.}
    \label{fig:my_label}
\end{figure}
\end{document}