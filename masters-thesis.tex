% Packages (fold)
\RequirePackage{lmodern}
\documentclass[12pt, oneside, extrafontsizes]{memoir}  

\setstocksize{11in}{8.5in}
\settrimmedsize{11in}{8.5in}{*}
\settrims{0in}{0in}
\setlrmarginsandblock{38mm}{25mm}{*}
\setulmarginsandblock{25mm}{25mm}{*}
\setheadfoot{13pt}{26pt}
\setheaderspaces{*}{13pt}{*}
\checkandfixthelayout
\DoubleSpacing
\setsecnumdepth{subsubsection}
\headstyles{default}
\chapterstyle{ell}
\setsecheadstyle{\scshape\LARGE\raggedright}

\usepackage[colorlinks,bookmarksnumbered,bookmarksdepth=subsubsection,unicode=true]{hyperref}
\newsubfloat{figure}  % must follow hyperref
\hypersetup{
pdfauthor = {Pierre Thodoroff},
pdftitle = {Temporal Regularization for Markov Decision Process},
pdfsubject = {Subject},
pdfkeywords = {reinforcement learning},
pdfcreator = {LaTeX with the hyperref package},
pdfproducer = {},
linkcolor = [HTML]{000000},
citecolor = [HTML]{0000FF},
}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{showidx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{bbm}
\usetikzlibrary{shapes,arrows}
\usepackage{standalone}
\usepackage{amsmath,algorithmic,algorithm,setspace}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{wrapfig}
% Custom commands

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollaries}{Corollary}
\newtheorem{property}{Property}
\newtheorem*{remark}{Remark}
\usetikzlibrary{shapes,arrows}
\usepackage{standalone}
\newcommand{\s}{\mathcal{S}}
\newcommand{\p}{\mathcal{P}}
\newcommand{\pol}{\pi}
\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Q}{Q}
\newcommand{\Vr}{\V_{\param}}
\newcommand{\V}{V}
\newcommand{\regT}{\T^{\pol}_{\param}}
\newcommand{\regTo}{\T^{*}_{\param}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\dr}{\widetilde{\delta}}
\newcommand{\qr}{\widetilde{Q}}
\newcommand{\pit}{\widetilde{\pi}}
\newcommand{\param}{\beta}
\newcommand{\vr}{V^{\param}_{\theta, \omega}}
\newcommand{\vrb}{V^{\param}_{\theta,\omega}}
\newcommand{\Vb}{V_{\theta,\omega}}
\newcommand{\Vt}{V_{\theta}}
\newcommand{\expect}{\mathop{\mathbb{E}}}
\newcommand{\RE}{r}
\newcommand{\Vm}{V_{\text{max}}}
\newcommand{\Rm}{R_{\text{max}}}
\newcommand{\Tvan}{\T^{\infty}_{\param}}
\newcommand{\Ibeta}{I_{\param}}


\makeindex

\usepackage[backend=biber, citestyle=authoryear, bibstyle=authoryear, isbn=false, url=false, doi=false, eprint=false, natbib=true, sorting=nty, uniquename=init]{biblatex}
\addbibresource{library.bib}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title (fold)
\pretitle{\begin{center}\cftchapterfont\huge}
\posttitle{\end{center}}
\preauthor{\begin{center}\huge}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\end{center}}

\title{Exploiting trajectory information in Reinforcement Learning}
\author{Pierre Thodoroff}
\date{\today}
\renewcommand\maketitlehookb{
\vfill
}
\renewcommand\maketitlehookc{
\vfill
\begin{center}
{
\large
Computer Science\\
McGill University, Montreal
}
\end{center}
\vspace{10mm}
}
\renewcommand\maketitlehookd{
\vspace{10mm}
A thesis submitted to McGill University in partial fulfilment of the requirements of
the degree of Master of Science.
\copyright Pierre Thodoroff; \today.
}
% Title (end)

\begin{titlingpage}
\maketitle
\end{titlingpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ackowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
%\pagenumbering{roman}
%\renewcommand{\abstractname}{Dedication}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ackowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{roman}
\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
TODO
Need to talk about nishanth here ? To check
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\renewcommand{\abstractname}{Abstract}
\begin{abstract}
Model-free Reinforcement Learning (RL) is a widely used framework for sequential decision making in many domains such as robotics and video games. However, its use in the real-world remains limited due, in part, to the high variance of value function estimates, leading to poor sample complexity. 

The problem of disentangling signal from noise in sequential domains is not specific to Reinforcement Learning and has been extensively studied in the Supervised Learning literature. In this thesis we propose to view the value function through the trajectory as a Time-Series. We leverage the Time-Series literature to propose methods aimed at reducing the variance of value function estimates. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abrégé
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\renewcommand{\abstractname}{Résumé}
\begin{abstract}
L'apprentissage par renforcement est un model d'apprentissage fréquemment utilise  pour modelise les problems de decisions sequentielles comme la robotique et les jeux videos. Cependent son utilisation dans le monde reele reste limite a cause de la forte variance des estimation de valeur, resultant 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\setcounter{tocdepth}{2}
\tableofcontents
%\newpage
%\listoffigures
%\listofalgorithms

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}
\chapter{Introduction}
\input{chapters/introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Markov Chains}
\label{chap:markov}
\input{chapters/markov}

\chapter{Reinforcement Learning}
\label{chap:reinforcement}
\input{chapters/reinforcement}


\chapter{Temporal Regularization}
\label{chap:temporal}
\input{chapters/temporal}


\chapter{Recurrent Learning}
\label{chap:recurrent}
\input{chapters/recurrent}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\input{chapters/conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliography{library}
%\bibliographystyle{plain}
\printbibliography
\end{document}