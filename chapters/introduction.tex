Since the inventions of computers, their ultimate goal is to display human level \emph{intelligence}. This quest started with the design of the Turing test in 1953 by Alan Turing. In the 1950's, many mathematical framework have been proposed \citep{hayes1985rule}[CITATION] to attempt to reproduce human intelligence. [RULE BASED VERSUS STATISTICS] In STATISTICS/ML/MATH.. The various attempt to address the problem of sequential decision making can be classified under two different paradigm: optimal control and trial and error.

 Optimal control attempts to find an \emph{optimal policy} for a dynamical system according to some \emph{optimality criterion}.  The field of optimal control was pioneered by Richard Bellman with the concepts of Dynamic Programming and the Bellman equation[CITATION HERE]. The bellman equation remains at the center of most Reinforcement algorithm nowadays and many other fields such as economic theory[CITATION HERE]. However many of the optimal control methods relied on the knowledge of the dynamics of the system. In contrast Trial and Error took essence from animals and psychology. The main idea was to use experience from the real world to drive the behavior towards \emph{desirable states}. This thread focused more on sampled experience from the environment to learn an optimal behaviour. When referring to Reinforcement Learning, the community often refers to the latter thread, however both can be seen as different strand of RL.

 In the Trial and Error thread, the core idea is to estimate the \emph{value} of each state of the world and direct the agent towards good state \emph{based on a stream of experience}. The value of a state describe the expected future return from this state onwards, under the current \emph{policy}. The goal of Reinforcement Learning is two-fold, one learning a value function describing how \emph{good} each state is, two learning to control an agent towards those \emph{good} states. In RL, one can also consider 2 paradigm: model-free or model-based. Model-based RL attempts to model the environment and behave optimally according to that model. In contrast model-free RL solely relies on samples obtained to derive and optimal behaviour. Intuitively model-based RL is a more appealing solution however learning a model of the environment can be complex. In practice it has shown to be less sample efficient than model-free RL[CITATION HERE]. In this thesis we focus on model free reinforcement learning.
   

In recent years, model-free RL has shown promises in many domains such as  such as robotics \citep{kober2013reinforcement,abbeel2010autonomous} and video games \citep{vinyals2017starcraft,mnih2013playing,mnih2016asynchronous}. It is also used in some real-life applications such as hydro control \citep{grinberg2014optimizing} and power grid management \citep{franccois2016deep}.However, model-free RL use in the real-world remains limited due, in part, to the high variance of value function estimates \citep{greensmith2004variance}, leading to poor sample complexity \citep{glascher2010states,kakade2003sample}. This phenomenon is exacerbated by the noisy conditions of the real-world \citep{fox2015taming,pendrith1994reinforcement}. Real-world applications remain challenging as they often involve noisy data such as sensor noise and partially observable environments.whether due to randomness in data collection, effects of initial conditions, complexity of learner function class, hyper-parameter configuration, or sparsity of the reward signal \citep{henderson2017deep}.
 
The problem of disentangling signal from noise in sequential domains is not specific to Reinforcement Learning and has been extensively studied in the Supervised Learning literature. In particular Time-Series analysis is a set of methods for extracting meaningful statistics from temporal data. It had numerous application from .. to .... The most basic, widely used model is called exponential smoothing used to estimate moving average.

 In this work, we leverage ideas from time series literature(in particular exponential smoothing) by viewing the value function through the trajectory as a time series. MORE HERE 

Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some (smaller) bias.  Regularization typically takes the form of smoothing over the observation space to reduce the complexity of the learner's hypothesis class. In the RL setting, we have an interesting opportunity to consider an alternative form of regularization, namely temporal regularization.  Effectively, temporal regularization considers smoothing over the trajectory, whereby the estimate of the value function at one state is assumed to be related to the value function at the state(s) that typically occur before it in trajectories. This structure arises naturally out of the fact that the value at each state is estimated using the Bellman equation. The standard Bellman equation clearly defines the dependency between value estimates. In temporal regularization we amplify this dependency by making each state depend more strongly on estimates of \emph{previous} states as opposed to multi-step that considers future states.  However 2 setting smoothing coefficient and decide when to smooth
However, exponential smoothing along the trajectory can result in a bias when the value function changes dramatically through the trajectory (non-stationarity). This bias could be a problem if the environment encounters sharp changes, such as falling of a cliff, and the estimates are heavily smoothed.

Instead of modifying the target we estimate the value functions directly using exponential smoothing We propose Recurrent Value Functions (RVFs): an exponential smoothing of the value function. The value function of the current state is defined as an exponential smoothing of the values of states visited along the trajectory where the value function of past states are summarized by the previous RVF. To alleviate the "falling of a cliff" issue, we propose to use exponential smoothing on value functions using a trainable state-dependent emphasis function which controls the smoothing coefficients. The emphasis function identifies important states in the environment. An important state can be defined as one where \emph{its value differs significantly from the previous values along the trajectory}. For example, when falling off a cliff, the value estimate changes dramatically, making states around the cliff more salient.

In chapter 1 we explaon markov concept. In 2 RL concept in 3 we introduce the concept of temporal regularization. Then we..
This paper proposes a class of temporally regularized value function estimates. We discuss properties of these estimates, based on notions from Markov chains, under the policy evaluation setting and extend the notion to the control case. Our experiments show that temporal regularization effectively reduces variance and estimation error in discrete and continuous MDPs.  The experiments also highlight that regularizing in the time domain rather than in the spatial domain allows more robustness to cases where state features are mis-specified or noisy, as is the case in the Atari domains.
In chapter 4 we consider RVF
To summarize the contributions of this work, we introduce RVFs to estimate the value function of a state by exponentially smoothing the value estimates along the trajectory. RVF formulation leads to a natural way of learning an emphasis function which mitigates the bias induced by smoothing. We provide an asymptotic convergence proof in tabular settings by leveraging the literature on asynchronous stochastic approximation \citep{tsitsiklis1994asynchronous}. Finally, we perform a set of experiments to demonstrate the robustness of RVFs with respect to noise in continuous control tasks and provide a qualitative analysis of the learned emphasis function which provides interpretable insights into the structure of the solution.


% TODO write the intro
%   Model-free Reinforcement Learning became more important with the discovery of the Temporal Difference algorithm by Sutton in 19881. Temporal difference led decades of innovation and successes in reinforcement learning and neighbouring field [CITATION HERE]. It is still to date one of the central algorithm behind many reinforcement algorithm [CITATION HERE]. 
%%%%%%%%%%%%%%%%%%%%%%%%


