Regularization in Machine Learning is a cornerstone. Most models can be seen as bias/variance trade-off.
\section{Regularization}
Regularization in RL has been considered in several different perspectives. In this section we discuss the most popular one. Other works focus on regularizing the changes in policy directly. Those approaches are often based on entropy methods (\cite{neu2017unified,schulman2017proximal,bartlett2009regal}).
Explicit regularization in the temporal space has received much less attention.
% Though it has been shown that exploiting \emph{temporal coherence} can benefit to tracking algorithms and thus help in meta-learning~\cite{sutton2007role}, this remains far from the setting considered in this work.
Temporal regularization in some sense may be seen as a ``backward'' multi-step method (\cite{sutton1998reinforcement}). 
\subsection{Spatial Regularization}
 One line of investigation focuses on regularizing the features learned on the state space (\cite{massoud2009regularized,petrik2010feature,pazis2011non,farahmand2011regularization,liu2012regularized,harrigan2016deep}). These approaches assume that nearby states in the state space have similar value.


\subsubsection{Regularization of the function approximation}
\cite{farahmand2011regularization}
This thesis studies the reinforcement learning and planning problems that are modeled by a discounted Markov Decision Process (MDP) with a large state space and finite action space. We follow the value-based approach in which a function approximator is used to estimate the optimal value function. The choice of function approximator, however, is nontrivial, as it depends on both the number of data samples and the MDP itself. The goal of this work is to introduce flexible and statistically-efficient algorithms that find close to optimal policies for these problems without much prior information about them. The recurring theme of this thesis is the application of the regularization technique to design value function estimators that choose their estimates from rich function spaces. We introduce regularization-based Approximate Value/Policy Iteration algorithms, analyze their statistical properties, and provide upper bounds on the performance loss of the resulted policy compared to the optimal one. The error bounds show the dependence of the performance loss on the number of samples, the capacity of the function space to which the estimated value function belongs, and some intrinsic properties of the MDP itself. Remarkably, the dependence on the number of samples in the task of policy evaluation is minimax optimal. We also address the problem of automatic parameter-tuning of reinforcement learning/planning algorithms and introduce a complexity regularization-based model selection algorithm. We prove that the algorithm enjoys an oracle-like property and it may be used to achieve adaptivity: the performance is almost as good as the performance of the unknown best parameters. Our two other contributions are used to analyze the aforementioned algorithms. First, we analyze the rate of convergence of the estimation error in regularized least-squares regression when the data is exponentially beta-mixing. We prove that up to a logarithmic factor, the convergence rate is the same as the optimal minimax rate available for the i.i.d. case. Second, we attend to the question of how the errors at each iteration of the approximate policy/value iteration influence the quality of the resulting policy. We provide results that highlight some new aspects of these algorithms.

\subsection{Policy Regularization}
There exists several ways to regularize policy gradient method. The most comonly used in recent research is called entropy regularization \cite{neu2017unified,schulman2017proximal,bartlett2009regal}.
\subsubsection{Entropy Regularization}
In policy gradient method, one successful approach consists of regularizing the entropy of your policy distribution \cite{neu2017unified}. Policy gradient methods will tend to converge to distributions with low entropy. By adding an entropy bonus, it encourages the policy to explore and often yields much better performance.
\begin{equation}
\begin{split}
    \pol &= \expect_{\pol}{r} + R(\pol)\\
    R(\pol) = \text{entrop}
\end{split}
\end{equation}
This sets of work \cite{schulman2017proximal,schulman2015trust} considers limiting the update of the policy gradient at each step to guarantee \emph{coherent} behavior. 


\subsection{Temporal Regularization}
Though no work explicitely considers the bias and variance induced of considering the past estimates, several work attempts to exploit this relationship to better performance.
\subsubsection{Natural value approximator}
The closest work to ours is possibly \cite{xu2017natural}, where they define natural value approximator by projecting the previous states estimates by adjusting for the reward and $\gamma$. Their formulation, while sharing similarity in motivation, yields different theory and algorithm in practice.
\subsubsection{Backward bootstrapping}
Backward bootstrapping method's can be seen as regularizing in feature space based on temporal proximity \cite{sutton2009fast,li2008worst,baird1995residual}
\subsection{Deliberation cost}
There exists between temporal regularization and option. Indeed having a high deliberation cost \cite{harb2017waiting} means that you smooth out your problem by having longer action. This can be seen as a bias variance trade off. 
\subsubsection{Temporal coherence}
Though it has been shown that exploiting \emph{temporal coherence} can benefit to tracking algorithms and thus help in meta-learning~\cite{sutton2007role}, this remains far from the setting considered in this work.


